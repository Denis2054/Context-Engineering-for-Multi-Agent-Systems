{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "H100",
      "toc_visible": true,
      "authorship_tag": "ABX9TyMglIVPvqSb/RqmYQ8EmKJR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a7a6a11d79174a3da6f0dd6c42ba94ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b2090d818084c4c823d50c6808103f8",
              "IPY_MODEL_333a3035c73a4ac292f61dd42a933547",
              "IPY_MODEL_e58164781b0f4ac1adb9cc6ed979548a"
            ],
            "layout": "IPY_MODEL_212face83816458eaa53011e1c594800"
          }
        },
        "7b2090d818084c4c823d50c6808103f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b98d170751f4b4c9ef690f27aec8f81",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_5957468d431346bfab3d9518e78c22cc",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "333a3035c73a4ac292f61dd42a933547": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_566c527262b54e7ea811b21185772b80",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2803e17f01994e2882379cfe90b6b7b8",
            "value": 4
          }
        },
        "e58164781b0f4ac1adb9cc6ed979548a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_463b8b5435524270ba218831d6950acd",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_b72d9753f316411d89fde8c5c557cf45",
            "value": "‚Äá4/4‚Äá[05:16&lt;00:00,‚Äá68.89s/it]"
          }
        },
        "212face83816458eaa53011e1c594800": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b98d170751f4b4c9ef690f27aec8f81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5957468d431346bfab3d9518e78c22cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "566c527262b54e7ea811b21185772b80": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2803e17f01994e2882379cfe90b6b7b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "463b8b5435524270ba218831d6950acd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b72d9753f316411d89fde8c5c557cf45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Denis2054/Context-Engineering-for-Multi-Agent-Systems/blob/main/sovereign_ai/DeepSeek%E2%80%91R1_Sovereign_AI_Guide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **DeepSeek‚ÄëR1 Sovereign AI Guide**\n",
        "*Implementing the Open‚ÄëSource DeepSeek‚ÄëR1‚ÄëDistill‚ÄëLlama‚Äë8B Model*\n",
        "\n",
        "üìå **Copyright 2025-2026, Denis Rothman**  "
      ],
      "metadata": {
        "id": "L5nF0MqowHjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üåç Why Open‚ÄëSource LLMs Enable Sovereign, Strategic AI**\n",
        "\n",
        "Using an open‚Äësource model such as **_DeepSeek‚ÄëR1‚ÄëDistill‚ÄëLlama‚Äë8B_** gives organizations full **_sovereignty_** over their AI infrastructure. Because the model runs entirely on hardware you control‚Äîwhether in Google Drive, a local workstation, or a private cloud‚Äîevery prompt, dataset, and output remains **_inside your own environment_**. This eliminates reliance on external APIs and ensures that sensitive information never leaves your secured systems.\n",
        "\n",
        "Open‚Äësource deployment also provides **_long‚Äëterm independence_** for strategic projects. You are free to customize, optimize, fine‚Äëtune, or extend the model without vendor lock‚Äëin, usage limits, or unpredictable pricing changes. This level of autonomy is essential for government agencies, research institutions, and enterprises that require stable, transparent, and fully auditable AI behavior.\n",
        "\n",
        "Finally, models like **_DeepSeek‚ÄëR1_** support **_regulatory compliance_** and **_data‚Äëgovernance requirements_** by allowing teams to enforce their own security policies end‚Äëto‚Äëend. Whether the goal is confidential R&D, legal analysis, industrial planning, or national‚Äëlevel digital sovereignty, open‚Äësource LLMs offer a robust foundation for building trustworthy, mission‚Äëcritical AI systems.\n"
      ],
      "metadata": {
        "id": "OW5yCFu8vc8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Motivation** This notebook serves as a **Sovereign AI Proof of Concept**. It demonstrates how to deploy a high-reasoning, open-source model, in this case,  **DeepSeek-R1-Distill-Llama-8B**, in a fully controlled environment. By running this locally, organizations ensure that strategic data never leaves their infrastructure, fulfilling the requirements for *mission-critical and regulated industries*."
      ],
      "metadata": {
        "id": "tDJgcTTk8Fcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **üöÄ Installing and running DeepSeek-R1-Distill-Llama-8B**  \n",
        "\n",
        "This notebook provides a **step-by-step guide** on how to **download and run DeepSeek-R1-Distill-Llama-8B** locally in **Google Drive**.  The version downloaded is an open-source distilled version of DeepSeek-R1 provided by  unsloth, an LLM accelerator,  on Hugging Face :https://unsloth.ai/\n",
        "\n",
        "If you don't want to use Google Drive, you can install the artefacts on a local machine, server or cloud server.\n",
        "\n",
        "### **üîπ How to Get Started**  \n",
        "1Ô∏è‚É£ **Install the model's artifacts** ‚Üí Set `install_deepseek=True` and run all cells.  \n",
        "2Ô∏è‚É£ **Restart the session** ‚Üí Disconnect and start a new session.  \n",
        "3Ô∏è‚É£ **Re-run the model** ‚Üí Set `install_deepseek=False` and run all cells again.  \n",
        "4Ô∏è‚É£ **Interact with the model** ‚Üí Use it in a prompt session!  \n",
        "\n",
        "‚ö†Ô∏è **System Requirements**  \n",
        "‚úÖ **GPU** ‚Äì Minimum **16GB** VRAM required.  \n",
        "‚úÖ **Google Drive Space** ‚Äì At least **20GB** free space.  \n",
        "üìå **Educational Use Only** ‚Äì For production, deploy artifacts on a **local or cloud server**.\n",
        "\n",
        "---\n",
        "\n",
        "## **üìñ Table of Contents**  \n",
        "\n",
        "### **1Ô∏è‚É£ Setting Up the DeepSeek Environment (Hugging Face)**  \n",
        "‚úÖ Checking GPU Activation  \n",
        "üìÇ Mounting Google Drive  \n",
        "‚öôÔ∏è Installing the Hugging Face Environment  \n",
        "üîÑ Ensuring `install_deepseek=True` for First Run  \n",
        "üìå Checking Transformer Version  \n",
        "\n",
        "### **2Ô∏è‚É£ Downloading DeepSeek-R1-Distill-Llama-8B**  \n",
        "üìÇ Verifying Download Path  \n",
        "\n",
        "### **3Ô∏è‚É£ Running a DeepSeek Session**  \n",
        "üîÑ Setting `install_deepseek=False` for Second Run  \n",
        "üìå Model Information  \n",
        "üí¨ Running an Interactive Prompt Session  \n",
        "\n",
        "---\n",
        "\n",
        "### **üí° Ready to Use DeepSeek?**  \n",
        "Follow the **installation steps**, ensure you have the required **hardware**, and launch your **interactive AI session** üöÄ"
      ],
      "metadata": {
        "id": "dB7uI-BJ94pW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was developed in Google Colab. Colab includes many pre-installed libraries and sets `/content/` as the default directory, meaning you can access files directly by their filename if you wish (e.g., `filename` instead of needing to specify `/content/filename`). This differs from local environments, where you'll often need to install libraries or specify full file paths."
      ],
      "metadata": {
        "id": "Nb0mHg3p4yBV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setting up DeepSeek Hugging Face environment"
      ],
      "metadata": {
        "id": "mA0_omNcKCw4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Set the *installation toggle*. Use `True` for the initial setup to download artifacts to your persistent storage (e.g., Google Drive), and `False` for subsequent inference sessions."
      ],
      "metadata": {
        "id": "N8QnpT3S8iq2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set install_deepseek to True to download and install R1 Distill Llama 8B locally\n",
        "# Set install_deepseek to False to run an R1 session\n",
        "install_deepseek=False"
      ],
      "metadata": {
        "id": "81qmq4cJ65_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Checking GPU activation"
      ],
      "metadata": {
        "id": "2WX2FRUyvnmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verify the hardware accelerator. For industrial-grade performance in *seconds*, not *minutes*, this notebook is optimized for **NVIDIA H100(Hopper architecture)** with **HBM3(High Bandwidth Memory 3)** is the third generation of ultra‚Äëhigh‚Äëspeed stacked memory used in advanced GPUs and AI accelerators.\n",
        "\n",
        "The following command confirms the available `VRAM` and driver status."
      ],
      "metadata": {
        "id": "SKH8xz8E8urE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "8Lnpx8Ywvkqu",
        "outputId": "c2bb7e14-f74a-4165-acee-c68f30225b38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Feb 11 16:38:19 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  NVIDIA H100 80GB HBM3          Off |   00000000:04:00.0 Off |                    0 |\n",
            "| N/A   36C    P0             69W /  700W |       0MiB /  81559MiB |      0%      Default |\n",
            "|                                         |                        |             Disabled |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount Google Drive"
      ],
      "metadata": {
        "id": "87z54INBvyUJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mount persistent storage to maintain a local `model bank`. This prevents the need to re-download the *8B parameter model* (approx. 15GB-20GB) in every new session."
      ],
      "metadata": {
        "id": "fg2p1fP-9UTL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJAvtheKuudm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12f6bc12-9fa1-415c-865d-b262e9099d48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Redirect the Hugging Face cache to your mounted storage. This ensures the model shards are retrieved from your private environment, maintaining architectural sovereignty."
      ],
      "metadata": {
        "id": "87YZbFXi9jiO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Define the cache directory in your Google Drive\n",
        "cache_dir = '/content/drive/MyDrive/genaisys/HuggingFaceCache'\n",
        "\n",
        "# Set environment variables to direct Hugging Face to use this cache directory\n",
        "os.environ['TRANSFORMERS_CACHE'] = cache_dir"
      ],
      "metadata": {
        "id": "TzzNqXIRCSxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installation Hugging Face environment\n",
        "\n"
      ],
      "metadata": {
        "id": "hk_OMj3Xv7H6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the specific version of the *Transformers library* required for *DeepSeek-R1* compatibility. This ensures stability across different cloud or local environments."
      ],
      "metadata": {
        "id": "lMEUUvLE9vrt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.48.3"
      ],
      "metadata": {
        "id": "07XwZO2Bx1sZ",
        "outputId": "5324d1c1-286e-4bcb-aeff-dd0fd215d683",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.48.3\n",
            "  Downloading transformers-4.48.3-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/44.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (3.20.3)\n",
            "Collecting huggingface-hub<1.0,>=0.24.0 (from transformers==4.48.3)\n",
            "  Downloading huggingface_hub-0.36.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (26.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (2.32.4)\n",
            "Collecting tokenizers<0.22,>=0.21 (from transformers==4.48.3)\n",
            "  Downloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers==4.48.3) (4.67.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (1.2.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers==4.48.3) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers==4.48.3) (2026.1.4)\n",
            "Downloading transformers-4.48.3-py3-none-any.whl (9.7 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m108.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.36.2-py3-none-any.whl (566 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m566.4/566.4 kB\u001b[0m \u001b[31m63.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tokenizers-0.21.4-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m159.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface_hub 1.4.0\n",
            "    Uninstalling huggingface_hub-1.4.0:\n",
            "      Successfully uninstalled huggingface_hub-1.4.0\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.22.2\n",
            "    Uninstalling tokenizers-0.22.2:\n",
            "      Successfully uninstalled tokenizers-0.22.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 5.0.0\n",
            "    Uninstalling transformers-5.0.0:\n",
            "      Successfully uninstalled transformers-5.0.0\n",
            "Successfully installed huggingface-hub-0.36.2 tokenizers-0.21.4 transformers-4.48.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.DeepSeek download\n",
        "\n"
      ],
      "metadata": {
        "id": "CMDClDdHJ1nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize the model loading sequence. We use `device_map=auto `to optimize memory distribution across the H100‚Äôs HBM3 memory. On SOTA hardware, this setup enables the model to remain resident in VRAM for near-instant response."
      ],
      "metadata": {
        "id": "S7SBVy_J966Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time\n",
        "if install_deepseek==True:\n",
        "   # Record the start time\n",
        "  start_time = time.time()\n",
        "\n",
        "  model_name = 'unsloth/DeepSeek-R1-Distill-Llama-8B'\n",
        "  # Load the tokenizer and model\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_name, device_map='auto', torch_dtype='auto')\n",
        "\n",
        "    # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "FT1zO4ShzzON",
        "outputId": "e6d26f54-ab0f-4c92-c205-2f46be5ec676",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/transformers/utils/hub.py:128: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Audit the local file system to confirm all model shards and snapshots have been correctly saved to your *sovereign storage.*"
      ],
      "metadata": {
        "id": "gVQ3mJKc-tZ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==True:\n",
        " !ls -R /content/drive/MyDrive/genaisys/HuggingFaceCache"
      ],
      "metadata": {
        "id": "zz0hLeEhJTt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.DeepSeek-R1-Distill-Llama-8B session"
      ],
      "metadata": {
        "id": "mPFjy90U2gcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the model"
      ],
      "metadata": {
        "id": "zMyxVpqI4tdG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prepare the inference session. By setting `local_files_only=True`, we guarantee that the system is operating in a disconnected, sovereign mode with no external calls to Hugging Face during execution."
      ],
      "metadata": {
        "id": "06AUIxyB-14J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "if install_deepseek==False:\n",
        "  # Define the path to the model directory\n",
        "  model_path = '/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/71f34f954141d22ccdad72a2e3927dddf702c9de'\n",
        "\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "  # Load the tokenizer and model from the specified path\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model_path, local_files_only=True)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto', torch_dtype='auto', local_files_only=True)\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "  print(f\"Time taken to load the model: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "uyWIUDSt3_2k",
        "outputId": "0de1c0ef-9f6e-4ad2-8424-5a1b90fea4ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "a7a6a11d79174a3da6f0dd6c42ba94ce",
            "7b2090d818084c4c823d50c6808103f8",
            "333a3035c73a4ac292f61dd42a933547",
            "e58164781b0f4ac1adb9cc6ed979548a",
            "212face83816458eaa53011e1c594800",
            "5b98d170751f4b4c9ef690f27aec8f81",
            "5957468d431346bfab3d9518e78c22cc",
            "566c527262b54e7ea811b21185772b80",
            "2803e17f01994e2882379cfe90b6b7b8",
            "463b8b5435524270ba218831d6950acd",
            "b72d9753f316411d89fde8c5c557cf45"
          ]
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a7a6a11d79174a3da6f0dd6c42ba94ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken to load the model: 337.14 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Review the model configuration. Note the max_position_embeddings and `torch_dtype`. On `H100`, we utilize bfloat16 to maximize throughput without sacrificing reasoning precision."
      ],
      "metadata": {
        "id": "Wxzp0f6k-_8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==False:\n",
        "  print(model.config)"
      ],
      "metadata": {
        "id": "jSgiBU2m8rJJ",
        "outputId": "b0cf9a8d-99cf-4460-80a2-60222e8c4f95",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LlamaConfig {\n",
            "  \"_attn_implementation_autoset\": true,\n",
            "  \"_name_or_path\": \"/content/drive/MyDrive/genaisys/HuggingFaceCache/models--unsloth--DeepSeek-R1-Distill-Llama-8B/snapshots/71f34f954141d22ccdad72a2e3927dddf702c9de\",\n",
            "  \"architectures\": [\n",
            "    \"LlamaForCausalLM\"\n",
            "  ],\n",
            "  \"attention_bias\": false,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 128000,\n",
            "  \"eos_token_id\": 128001,\n",
            "  \"head_dim\": 128,\n",
            "  \"hidden_act\": \"silu\",\n",
            "  \"hidden_size\": 4096,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 14336,\n",
            "  \"max_position_embeddings\": 131072,\n",
            "  \"mlp_bias\": false,\n",
            "  \"model_type\": \"llama\",\n",
            "  \"num_attention_heads\": 32,\n",
            "  \"num_hidden_layers\": 32,\n",
            "  \"num_key_value_heads\": 8,\n",
            "  \"pad_token_id\": 128004,\n",
            "  \"pretraining_tp\": 1,\n",
            "  \"rms_norm_eps\": 1e-05,\n",
            "  \"rope_scaling\": {\n",
            "    \"factor\": 8.0,\n",
            "    \"high_freq_factor\": 4.0,\n",
            "    \"low_freq_factor\": 1.0,\n",
            "    \"original_max_position_embeddings\": 8192,\n",
            "    \"rope_type\": \"llama3\"\n",
            "  },\n",
            "  \"rope_theta\": 500000.0,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"bfloat16\",\n",
            "  \"transformers_version\": \"4.48.3\",\n",
            "  \"unsloth_fixed\": true,\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 128256\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompts\n"
      ],
      "metadata": {
        "id": "QTSNYBCb4vtt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define complex, domain-specific prompts (Legal and Industrial).\n",
        "\n",
        "These prompts include Strict Output Rules to test the model's ability to follow architectural constraints."
      ],
      "metadata": {
        "id": "-sQML8vx_JS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if install_deepseek==False:\n",
        "  prompt1 = \"\"\"\n",
        "Provide a clear, concise, and professional explanation of how a product designer can turn customer requirements for a traveling bag into a practical production plan.\n",
        "\n",
        "Strict Output Rules:\n",
        "- Do not show internal reasoning, chain-of-thought, or hidden thinking.\n",
        "- Do not describe what you are doing.\n",
        "- Do not restate or reflect on the instructions.\n",
        "- Do not explain your approach.\n",
        "- Do not comment on the task.\n",
        "- Do not use filler phrases such as ‚ÄúAlright, so the user wants‚Ä¶‚Äù.\n",
        "- Do not switch languages.\n",
        "- Output only the final answer in clean English.\n",
        "- Use bullet points only.\n",
        "- Each bullet point must be a single factual statement.\n",
        "- If you begin to generate internal reasoning, stop immediately and output only the final answer.\n",
        "\"\"\"\n",
        "  prompt2= \"\"\"\n",
        "Provide a clear, concise, and professional explanation of how a legal advisor should create a plan to defend a copyright issue in court.\n",
        "\n",
        "Strict Output Rules:\n",
        "- Do not show internal reasoning, chain-of-thought, or hidden thinking.\n",
        "- Do not describe what you are doing.\n",
        "- Do not restate or reflect on the instructions.\n",
        "- Do not explain your approach.\n",
        "- Do not comment on the task.\n",
        "- Do not use filler phrases such as ‚ÄúAlright, so the user wants‚Ä¶‚Äù.\n",
        "- Do not switch languages.\n",
        "- Output only the final answer in clean English.\n",
        "- Use bullet points only.\n",
        "- Each bullet point must be a single factual statement.\n",
        "- If you begin to generate internal reasoning, stop immediately and output only the final answer.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "t99pgv0IQ30m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Execute the inference. Benchmark Note: On the NVIDIA H100, this 8B reasoning model achieves inference in about 10 seconds (approx. 9.75s) for complex multi-step tasks. This proves that open-source sovereign AI can match the responsiveness of proprietary cloud APIs."
      ],
      "metadata": {
        "id": "HZtk70e5_Ynt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "if install_deepseek==False:\n",
        "  # Record the start time\n",
        "  start_time = time.time()\n",
        "\n",
        "\n",
        "  # Tokenize the input\n",
        "  inputs = tokenizer(prompt2, return_tensors='pt').to('cuda')\n",
        "\n",
        "  # Generate output with enhanced anti-repetition settings\n",
        "  outputs = model.generate(\n",
        "    **inputs,\n",
        "    max_new_tokens=500,\n",
        "    repetition_penalty=1.05,             # Increase penalty to 1.5 or higher\n",
        "    no_repeat_ngram_size=3,             # Prevent repeating n-grams of size 3\n",
        "    temperature=0.2,                    # Reduce randomness slightly\n",
        "    top_p=0.9,                          # Nucleus sampling for diversity\n",
        "    top_k=20,                            # Limits token selection to top-k probable tokens\n",
        "    eos_token_id=tokenizer.eos_token_id\n",
        "  )\n",
        "\n",
        "  # Decode and display the output\n",
        "  generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "  # Record the end time\n",
        "  end_time = time.time()\n",
        "\n",
        "  # Calculate the elapsed time\n",
        "  elapsed_time = end_time - start_time\n",
        "\n",
        "print(f\"Time taken for inference: {elapsed_time:.2f} seconds\")"
      ],
      "metadata": {
        "id": "Qt9GTrB34r2j",
        "outputId": "48941b69-c001-4aa4-e328-c753134ef4f1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Time taken for inference: 9.75 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Display the final response. This output represents the **Glass-Box** results: a factual, structured production or legal plan generated entirely within a secure, sovereign environment."
      ],
      "metadata": {
        "id": "C9oJPYrr_lFV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(generated_text)"
      ],
      "metadata": {
        "id": "L516j6MXttH5",
        "outputId": "e6e94536-d183-4b94-a393-79183807730c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Provide a clear, concise, and professional explanation of how a legal advisor should create a plan to defend a copyright issue in court.\n",
            "\n",
            "Strict Output Rules:\n",
            "- Do not show internal reasoning, chain-of-thought, or hidden thinking.\n",
            "- Do not describe what you are doing.\n",
            "- Do not restate or reflect on the instructions.\n",
            "- Do not explain your approach.\n",
            "- Do not comment on the task.\n",
            "- Do not use filler phrases such as ‚ÄúAlright, so the user wants‚Ä¶‚Äù.\n",
            "- Do not switch languages.\n",
            "- Output only the final answer in clean English.\n",
            "- Use bullet points only.\n",
            "- Each bullet point must be a single factual statement.\n",
            "- If you begin to generate internal reasoning, stop immediately and output only the final answer.\n",
            "- Ensure that each bullet is clear, precise, and directly related to the specifics of defending a copyright in court.\n",
            "- Avoid any markdown formatting.\n",
            "- Keep each bullet concise.\n",
            "\n",
            "Okay, so I need to figure out how a lawyer would create a defense plan for a copyright case. First, I should understand what the user is asking for. They want a clear and professional plan that a legal advisors can use in court. The output needs to follow strict rules, so no internal thoughts, just the final bullet points.\n",
            "\n",
            "I remember that when defending a Copyright issue, several key steps are involved. The lawyer should start by gathering all relevant evidence. That includes documents, communications, and any other materials related to copyright ownership. Then, they need to conduct a thorough investigation to understand the plaintiff's claims and the facts of the case.\n",
            "\n",
            "Next, legal research is crucial. The attorney should look into similar cases and applicable laws to find precedents that support their position. They also need to prepare a solid defense strategy, whether it's challenging the claim, proving fair use, or showing lack of originality.\n",
            "\n",
            "Drafting legal documents comes next. This includes motions, briefs, and affidavits. These papers need to be well-crafted to present the strongest arguments possible. The discovery process is also important, where the lawyer can request information from the opposing party to build a stronger case.\n",
            "\n",
            "Consulting with experts might be necessary, especially if there are technical aspects to the copyright claim. Expert testimony can provide valuable insights and strengthen the defense. The advocate should also prepare for court appearances, practicing their arguments and examining witnesses.\n",
            "\n",
            "Throughout this process, the lawyer must stay organized and communicate effectively with the client. Ensuring all deadlines are met and keeping the client informed helps manage expectations. Finally, the defense should be presented clearly and persuasively during trial.\n",
            "\n",
            "I need to structure these thoughts into bullet points without any explanations or filler. Each point should be a factual statement related to defending a court case involving copyright issues.\n",
            "</think>\n",
            "\n",
            "- Gather all relevant documentation, including copyright ownership records, contracts, and communication history.  \n",
            "- Conduct a thorough factual investigation to identify the scope of the alleged infringement and the plaintiff‚Äôs claims.  \n",
            "  \n",
            "- Perform legal research to identify applicable copyright laws, relevant case precedents, and defenses available (e.g., fair use).  \n",
            "- Prepare a defense strategy based on the specific facts ofthe case, such as challenging the validity of the copyright, proving lack oforiginality, or asserting fair\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üîç Deconstructing the Reasoning Output\n",
        "\n",
        "In this Proof of Concept run on an `NVIDIA H100`, we observe the raw output of the `DeepSeek-R1-Distill-Llama-8B` model. This output is composed of three distinct segments that require specific handling in a **Sovereign Context Engine**:\n",
        "\n",
        "**The Prompt Reflection (Post-Processing Required)**: Notice how the model initially restates the rules and its understanding of the legal task. In a production UI, this section is typically filtered out via string parsing or regex to maintain a professional interface.\n",
        "\n",
        "**The Reasoning Trace (The </think> Block)**: This is the 'Glass-Box' in action. The model explicitly weighs the steps of a copyright defense‚Äîfrom evidence gathering to expert testimony‚Äîbefore committing to a final answer. For Sovereign AI, this trace is your audit trail, providing 100% observability into how the AI reached its conclusion.\n",
        "\n",
        "**The Final Factual Output:** This is the clean, bulleted production plan the user requested. On the H100, this entire multi-stage cognitive process was completed in just 9.75 seconds, demonstrating that open-source models can deliver high-speed, verifiable results without external API dependencies.\n",
        "\n",
        "**Strategic Takeaway for Architects:**  When deploying this in a Multi-Agent System, your Orchestrator should be designed to capture the reasoning trace for your logs while delivering only the final cleaned text to the end-user. This ensures both operational transparency and a seamless user experience.\""
      ],
      "metadata": {
        "id": "fgHY0viqAXET"
      }
    }
  ]
}